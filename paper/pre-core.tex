\newcommand\ruledef{::=}
\newcommand\rulealt{\;|\;}
\newcommand\tyarr[2]{#1 \to #2}
\newcommand\tyany{\star}
\newcommand\cty[2]{#2 ! #1}
\newcommand\tynat{\mathrm{nat}}
\newcommand\tybool{\mathrm{bool}}
\newcommand\tyunit{\mathrm{unit}}
\newcommand\tystring{\mathrm{string}}
\newcommand\effany{\mbox{\tiny\FiveStarOpen}}
\newcommand\effop{\mathit{op}}
\newcommand\app[2]{#1\,#2}
\newcommand\lam[2]{\lambda\, #1 .\, #2}
\newcommand\cast[2]{#1 \,\MVAt\, #2}
\newcommand\tbox[2]{#1 \Uparrow #2}
\newcommand\blame{\mathrm{blame}}
\newcommand\perform[2]{#1\left(#2\right)}
\newcommand\handle[2]{\mathbf{handle}\,\{#1\}\,#2}
\newcommand\fori[3]{(#3)_{#1=1\dots #2}}
\newcommand\foriset[3]{\{#3\}_{#1=1\dots #2}}
\newcommand\treturn[1]{\mathbf{return}\,#1}
\newcommand\stepto{\longrightarrow}
\newcommand\stepsto{\stepto^\ast}
\newcommand\subst[3]{#1[#2/#3]}
\newcommand\context[2]{#1[#2]}
\newcommand\xC{\mathcal{C}}
\newcommand\xE{\mathcal{E}}
\newcommand\xP{\mathcal{P}}
\newcommand\bound[1]{\mathbf{bound}(#1)}
\newcommand\cid{\mathrm{id}}
\newcommand\commutel[3]{\mathbf{commutes}^l(#1,#2,#3)}
\newcommand\commuter[3]{\mathbf{commutes}^r(#1,#2,#3)}
\newcommand\cod[1]{\mathrm{cod}(#1)}
\newcommand\codeff[1]{\mathrm{eff}(#1)}
\newcommand\dom[1]{\mathrm{dom}(#1)}
\newcommand\typeof[1]{\mathrm{typeof}(#1)}
\newcommand\dprime{{\prime\prime}}

\newcommand\Term[3]{#1 \vdash #2 : #3}
\newcommand\TermP[4]{#1 \vdash #2 \le #3 : #4}

\newcommand\dhandler{{\color{red}\texttt{library}}}
\newcommand\shandler{{\color{blue}\texttt{library}}}
\newcommand\dclient{{\color{red}\texttt{client}}}
\newcommand\sclient{{\color{blue}\texttt{client}}}

\newcommand\eflog{{[\texttt{Log}]}}
\newcommand\efnone{[]}

\section{Introduction}

Computational effects are everywhere: state, concurrency, probability,
nondeterminism, input-output, exception handling. Algebraic effects with handlers,
introduced by \citet{plotkin-pretnar-2009},
% ^ TODO use this citation style everywhere
have seen rapid development in recent years as a way to support a wide range
of computational effects. They have inspired numerous libraries (TODO: name some of them), experimental
programming languages including Links, Eff, Koka, and Frank, and features
in programming languages including WebAssembly, OCaml, and Haskell.
(We give citations for these later.)

Type systems for tracking algebraic effects with handlers are a subject of intense study,
with prototypes appearing in Links, Eff, Koka, and Frank.
Meanwhile, the new features adopted in WebAssembly, OCaml, and Haskell
use algebraic effects and handlers without reflecting effects in types.
Further, virtually every program in existence makes some use of computational
effects. In the future, it will become vital to have some way for legacy
code with effects not reflected in the types to interact soundly with
new code that does have effects reflected in the types.

Gradual types, introduced by \citet{siek-taha-2006},
provide a model of how code with less precise types can interface soundly with
code with more precise types. Gradual types have been extensively studied for a wide
range of features, including some forms of computational effects. However,
until now noone has studied the combination of algebraic effects
with handlers and gradual types. We show that in fact this combination
is straightforward.

\paragraph{Gradual guarantee}
As type systems for algebraic effects with handlers emerge
it will become crucial to soundly integrate legacy code without effect types
with new code containing effect types. Of course the best solution would be
to write all of our code in languages that support effect types,
but that is unlikely to be feasible in practice.
Instead we will want to annotate legacy code with new types that reflect
the effects used, and to dynamically check that these annotations
are correct so that the new code may rely on the annotations as sound.

For example,
while Multicore OCaml features effect handlers, it lacks \emph{effect typing}:
its type system does not yet keep track of effects.
If a future version of OCaml is to feature effect typing,
it may well become a challenge to migrate existing code that could
not rely on effect typing. In the worst case there will be a
schism, splitting OCaml into OCaml-without-effect-typing and
OCaml-with-effect-typing. What we would like to see instead
is that code in OCaml-without-effect-typing and
code in OCaml-with-effect-typing can interact.

For instance, imagine a logging library, implemented as a higher-order function
which handles a $\eflog$ effect from its client and collects the logged output
in a string. In a type system that does not keep track of effects, we could assign the
library and the client the types shown in the first line of
\Cref{fig:app-example}(TODO: move figure in text), making no distinction between pure and effectful
functions.
In a type system that keeps track of effects, the types in the first line
expand to the types in the second line, making explicit the dynamic effect $\effany$
associated with each arrow: the functions may have some effects, but we
don't know which effects at compile time.
We could then provide more precise types,
shown in the third line of that figure,
making explicit the effects that the library and client consume and produce.

\begin{figure}[H]
$$
\begin{array}{l|l}
  \dhandler : (\tyunit \to \tyunit) \to \tystring & \dclient : \tyunit \to \tyunit \\
  \hphantom{\dhandler} : (\tyunit \to \tyunit\,!\,\effany) \to \tystring\,!\,\effany & \hphantom{\dclient} : \tyunit \to \tyunit\,!\,\effany \\
  \shandler : (\tyunit \to \tyunit\,!\,\eflog) \to \tystring \,!\, \efnone & \sclient : \tyunit \to \tyunit\,!\,\eflog
\end{array}
$$
  \caption{A library and a client with {\color{red}untyped} and {\color{blue}typed} effects. Unnanotated arrows (first line) implicitly produce the dynamic effect $\effany$ (second line)}
\label{fig:app-example}
\end{figure}

If the library and the client are large artifacts, it is desirable to
perform this migration from untyped code $\dhandler(\dclient)$ to typed code $\shandler(\sclient)$ gradually.
We may migrate the library first, yielding the intermediate program $\shandler(\dclient)$,
or we may migrate the client first, yielding $\dhandler(\sclient)$.
A desirable property for a gradual language to support such a migration is that all
four of those combinations (\Cref{fig:migration-example})
should compile and should yield the same answer when run.
Casts are inserted as required between components where effects are unknown at compile time
and components where effects are known. A cast from effect $\effany$ to a known effect $E$
checks at run time that the actual effects performed by the computation at run time are
a subset of the effects specified by $E$. A cast in the other direction,
from $E$ to $\effany$, requires no additional checks at run time.

\begin{figure}[H]
$$
\begin{array}{c|cc}
  & \dhandler & \shandler \\
  \hline
  \dclient & \dhandler(\dclient) & \shandler(\dclient) \\
  \sclient & \dhandler(\sclient) & \shandler(\sclient)
\end{array}
$$
\caption{The four possible combinations of typed or untyped library and client}
\label{fig:migration-example}
\end{figure}

To ensure that such a migration goes smoothly, the addition of correct type annotations
should not change the behavior of the program. This property
is called \emph{the gradual guarantee}~\citep{siek2015} or \emph{graduality}~\citep{new-ahmed-2018}.
As we make the types in the program more precise, some type errors may be
revealed, but when the program does run successfully, the output
should not change. The proof follows the usual approach:
we define an imprecision relation on types, extend it
to an imprecision relation on terms, and show that the imprecision relation on
terms is a simulation with regard to the reduction relation.

Above we've assumed that all code, with or without effect types,
makes use of algebraic effects and handlers.
We may also need to integrate legacy code that has effects---but
not algebraic effects and handlers---with new code which uses algebraic effects
and handlers and has effect types.
We hope that the techniques we develop might also be applicable to such a case,
although we don't examine it here.

\newcommand{\imp}{\le}
\newcommand{\pmi}{\ge}
\newcommand{\comp}{\sim}

\paragraph{Proof-relevant imprecision}
If $A$ and $B$ are types, we write $A \imp B$ (TODO make it square)
to indicate that $A$ is \emph{less imprecise} than $B$. (In the literature,
$\imp$ is often referred to as \emph{precision}, but \emph{imprecision} is more
accurate. It is also sometimes called \emph{naive subtyping}.)
a cast from $A$ to $B$ is called an \emph{upcast} if $A \imp B$
and a \emph{downcast} if $A \pmi B$.

Here following the New Perspective of~\citet{castagna-et-al-TODO} we restrict
all casts to be either upcasts or downcasts. Previous work instead permits
a cast from $A$ to $B$ whenever $A$ is \emph{compatible} with $B$,
written $A \comp B$. As is well known, $\imp$ is transitive whereas $\comp$
is not transitive. The two relations are connected in that $A \comp B$
if and only if there is a $C$ such that $A \pmi C$ and $C \imp B$.
Expressiveness is not compromised because any cast between
two compatible types can be rewritten as a downcast followed by an upcast.
The restriction to upcasts and downcasts enormously simplifies the
development: whereas previous proofs of blame safety need five
relations---compatibility, positive subtyping, negative subtyping,
subtyping, and imprecision---our development needs only imprecision
and the proof of blame safety becomes immediate.

\newcommand{\id}{\mathtt{id}}

Most developments of imprecision for gradual typing are \emph{proof irrelevant}:
for any $A$ and $B$, there is at most one proof that $A \imp B$.
However there is some advantage to a proof relevant development.
For instance the following two rules for imprecision overlap.
\begin{verbatim}
----------------
\id_A : A \imp A
\qquad
a : A \imp A'
b : B \imp B'
---------
(a \to b) : (A \to B) \imp (A' \to B')
\end{verbatim}
Hence the following two reduction rules for casts overlap.
\begin{verbatim}
cast \pm id_A V -> V
cast (\pm (a \to b)) V -> fun x -> cast \pm b (V (cast \mp a x))
\end{verbatim}
In previous developments, in the rule for $\id_A$ type $A$ would be restricted
to be a type other than a function type to avoid overlap with the rule for $a \to b$,
and as a result one could not use the efficient identity reduction to cast
a function type to itself. Here proof relevance enables us to use the
identity cast at function types, which offers a slight bump to efficiency.
More importantly we will also see that it introduces an interesting
categorical structure to the development. In particular, at some places
we will write (TODO)


(TODO Paragraph about blame safety)

(TODO paragraph about proof-relevant precision)

\paragraph{Contributions}

\begin{itemize}
  \item We present a calculus combining effect handlers with gradual typing,
    demonstrating the marriage of these features for the first time.
    We give the syntax, typing rules, and operational semantics,
    and prove preservation and progress.
  \item We also prove blame safety.
    Whereas previous proofs of blame safety need five
    relations---compatibility, positive subtyping, negative subtyping,
    subtyping, and imprecision---our proof needs only imprecision.
    (TODO did they make this remark in New Perspectives? probably no)
    (TODO add blame labels in Agda formaliz.)
  \item We prove that this language satisfies the gradual guarantee.
  \item We define a novel proof-relevant precision: the precision relation
    between terms is indexed by derivations of precision between their types,
    which helps simplify part of the proof of the gradual guarantee.
  \item Our work is mechanized in the Agda proof assistant.
\end{itemize}

\paragraph{Related work}

TODO Give the citations for all the main things that people want to know, anything mentioned above.
One sentence per relevant paper. Plotkin and Power for alg effects (Without handlers).

Algebraic effects were proposed by~\citet{plotkin-power-2001} as a framework
to represent effectful computations by operations and equations.
\citet{plotkin-pretnar-2009} then introduced handlers as a principled way
of interpreting such computations. Algebraic effects and handlers have been found
to be a powerful tool for writing effectful programs, spawning implementations
as libraries \citep{kammar2013handlers},
experimental languages (Eff~\citep{bauer-pretnar-2014}, Koka~\citep{leijen04}, Frank~\citep{dobedo}, Links~\citep{hillerstrom2016}),
and also as first-class features of industrial programming languages.


OCaml Multicore~\citep{dolan-2015} adds effect handlers to OCaml with
multicore concurrency as a primary application.
GHC version 9.6 includes new primitives for delimited continuations~\citep{ghc-delcont},
to enable efficient implementations of algebraic effects.
There is an ongoing proposal to introduce effect handlers as a unified primitve
for non-local control flow abstractions in WebAssembly~\citep{wasmfx}.

\citet{schwerter-2016} apply gradual types to the generic type-and-effect systems of \citet{marino-2009},
which do not support effect handlers.

TODO mention we don't handle polymorphic effects.
people are still confused about how parametricity and graduality interact.

% \paragraph{Gradual guarantee} If $M$ is more precise than $M'$ ($M \le M'$),
% and $M$ steps to a value $V$ ($M \to V$), then $M'$ steps to a value $V'$ which
% is less precise than $V$ ($M' \to V' \wedge V \le V'$).
\section{Gradual Eff}

\GEmetavars\\[0pt]
\GEgrammar\\[5.0mm]
\GEdefnss

\section{Cast calculus with handlers}

Syntax and operational semantics.

Our effect system is based on Eff~\cite{bauer2015programming}.

\subsection{Types}

We distinguish two kinds of types: value types $A$, $B$ (base types, function type, and
dynamic type $\star$) and computation types $P$, $Q$ which pair a value type with an effect (also called \emph{dirt}).

Effects may be static or dynamic. A static effect is a list of operation names,
restricting the operations that a computation may perform. The dynamic effect $\effany$
allows any operation to be performed.

\paragraph{Precision}
In a gradually typed language, types are intuitively ordered by the amount of
static information they give about their inhabitants. This ordering is called
naive subtyping, or \emph{precision}.
We write $a : A \le A'$ to mean that $a$ is a proof that $A$ is more precise than $A'$.
The least precise type is $\star$.
Effects are also ordered by precision, with the least precise effect being $\effany$.
A static effect is only related to itself and to $\effany$.
Effect subtyping can be added as an orthogonal feature.

A proof of precision $a : A \le A'$ serves as a cast between $A$ and $A'$:
either an upcast $+a : A \Rightarrow A'$ or a downcast $-a : A' \Rightarrow A$.

We chose to have a reflexivity rule for all types, which provides an
identity cast for all types. Another design choice would be to only introduce
that rule for base types, which would make proofs of precision $a : A \le A'$
unique for a given $A$ and $A'$. Although reflexivity would remain derivable
for all types, it would be less efficient as a cast.

\subsection{Terms}

Unlike Eff, we do not define fine-grained call-by-value calculus, as that leads
to complications when treating function casts in the proof of the gradual guarantee.

Thus, all terms are treated as computations, with a single judgement $\Gamma \vdash M : P$.
In particular, function applications $N\;M$ and operator applications $M + N$
sequence the computations for their operands, so they must all have the same
effect $E$.

Performing an operation $\mathbf{op}(M)$ requires $\mathbf{op}$ to be allowed
by the effect $E$: either $E$ is a static effect, then $\mathbf{op}$ must occur
in it, or $E$ is a dynamic effect, which allows all operations.

$\mathbf{handle}\,H\,M$ is a handler $H$ applied to a computation $M$.
A handler $H$ consists of one return clause which expects the result of the
handled computation, and zero or more operation clauses.

Value casts and effect casts respectively cast the value type and the effect of a term.
A value cast $[[pa : A => A']]$ transforms values to a more or less precise type.
An effect cast $[[pe : E => E']]$ asserts that a computation with effect $E$
only performs operations allowed by effect $E'$.

A box is a value of the dynamic type. It contains a ground type $G$
(which correspond to a type constructor) and a value of that type.
To cast a function from a non-ground type, $A \to P$ to $\star$,
we will first cast $A \to P$ to the ground type
$[[TYPE ( star -> star ! wstar )]]$ (by casting the domain and codomain individually),
which can then be boxed.

The $\mathbf{blame}$ constructor indicates a type error at run time.

\subsection{Operational semantics}

The behavior of casts depends on the structure of the cast.
A function cast $\pm (a \to b ! e)$
is split into casts on its domain $\mp a$ (note the inverted polarity), codomain $\pm b$, and effect $\pm e$.
An upcast into the dynamic type $+ (a \Uparrow G)$
recursively applies the cast $+ a$ and puts the result in a box.
A downcast from the dynamic type $- (a \Uparrow G)$ compares
the tag $G$ with the tag in the box; if the tags are equal, the value is
unboxed and the cast $- a$ is applied recursively,
otherwise we raise $\mathbf{blame}$.

Effect casts have reduction rules for values and disallowed effects,
but not for allowed effects. When an effect is allowed, it is simply
forwarded to the next handler or effect cast. In that sense,
an effect cast can be thought of as a handler which handles all operations
except the allowed ones.

The reduction rule for handlers is standard. A handler handles operations under an
evaluation context. The closest matching handler is selected by ensuring that
the evaluation context cannot already handle the operation.
Operations may be handled not only by handlers, but also by effect casts.



\begin{figure}
\[
\begin{array}{rcl}
\multicolumn{3}{l}{\fbox{$\bound{\xC}$}} \\
  \bound{\square} & = & \emptyset \\
  \bound{\context{\xP}{\xC}} & = & \bound{\xC} \\
  \bound{\handle{H}{\xC}} & = & \bound{H} \cup \bound{\xC} \\
  \bound{\cast{\xC}{\pm \cty{e}{a}}} & = & \bound{e} \cup \bound{\xC} \\
\multicolumn{3}{l}{\fbox{$\bound{e}$}} \\
  \bound{\pm\, \cid} & = & \emptyset \\
  \bound{+ \Uparrow E} & = & \emptyset \\
  \bound{- \Uparrow E} & = & \mathbf{Op} - E \\
  && \quad \makebox[0pt]{\text{where $\mathbf{Op}$ is the set of all operation symbols} }
\end{array}
\]
\[
\begin{array}{rcl}
\multicolumn{3}{l}{\fbox{$\bound{H}$}} \\
  \bound{\treturn{x}\mapsto N, \fori{i}{n}{\perform{\effop_i}{x,k} \mapsto N_i}}
    &=& \foriset{i}{n}{\effop_i}
\end{array}
\]
\caption{Operations bound by evaluation contexts and by effect casts}
\end{figure}

\paragraph{Soundness} If $[[empty |- M : A ! E]]$, then one of the following holds:
\begin{enumerate}
 \item $[[M --> M']]$ and $[[empty |- M' : A ! E]]$ for some $[[M']]$,
 \item $[[M]]$ is a value,
 \item $[[M]] = [[Ex [ op ( V ) ] ]]$ for some $[[Ex]]$, $[[op in E]]$, and $[[V]]$,
 \item $[[M]] = [[Ex [ blame ] ]]$ for some $[[Ex]]$.
\end{enumerate}

\section{Gradual guarantee}

Introducing type annotations in an untyped program $M^\prime$ transforms it into a more \emph{precise} program $M$.
This relation is denoted $M \le M^\prime$.
Intuitively, only adding type annotations should not change the behavior of the program.
But it may reveal type errors, either because $M'$ could not be fully well-typed
in the first place, or because type annotations were added incorrectly.
\footnote{Another approach is to ignore type annotations at run time, which leaves us with a very weak notion of type safety.}
That property is called the \emph{gradual guarantee}, and can be stated as follows.
If $M$ is more precise than $M^\prime$ and $M$ evaluates to $V$,
then $M^\prime$ evaluates to some $V^\prime$ less precise than $V$.

\newcommand\wwedge{\quad\wedge\quad}

\[
\begin{array}{rrcl}
  & \TermP{}{M}{M^\prime}{p} & \wedge & M \stepsto V
\\ \implies \exists V^\prime , & \TermP{}{V}{V^\prime}{p} & \wedge & M^\prime \stepsto V^\prime
\end{array}
\]

A converse property also holds: making a program more precise either results in a more precise value
or an error.

\[
\begin{array}{rrcl}
  & \TermP{}{M}{M^\prime}{p} & \wedge & M^\prime \stepsto V^\prime
  \\ \implies (\exists V , & \TermP{}{V}{V^\prime}{p} & \wedge & M \stepsto V) \vee M \stepsto [[blame]]
\end{array}
\]

This property is implied by the previous one when the reduction relation is deterministic, which it is for our calculus.

\begin{figure}
$$
\input{figures/gg}
$$
\caption{Diagram of the gradual guarantee}
\label{fig:gg}
\end{figure}

\subsection{Proof overview}

Simulation lemma (\Cref{fig:sim}): If $M^\prime$ is less precise than $M$, and $M$ takes one step to $N$,
then $M^\prime$ steps (in possibly many steps) to some $N^\prime$ less precise than $N$.

\[
\begin{array}{rrcl}
  & \TermP{}{M}{M^\prime}{p} & \wedge & M \stepto N
\\ \implies \exists N^\prime , & \TermP{}{N}{N^\prime}{p} & \wedge & M^\prime \stepsto N^\prime
\end{array}
\]

Catchup lemma (\Cref{fig:catchup}): if $M^\prime$ is less precise than a value $V$, then $M^\prime$ steps
to a value $V^\prime$ less precise than $V$.

\[
\begin{array}{rrcl}
  & \TermP{}{V}{M^\prime}{p} & &
\\ \implies \exists V^\prime , & \TermP{}{V}{V^\prime}{p} & \wedge & M^\prime \stepsto V^\prime
\end{array}
\]

$\beta$ lemma:

\[
\begin{array}{rccccc}
    & [[empty |- fun x . N <= fun x . N' : a0 ! e]]
    & \wedge & [[empty |- V <= V' : a ! e]]
    & \wedge & [[a0 ~~ ( a -> b ! e )]]
\\ \implies \exists [[M']],
    & [[empty |- N [ V / x ] <= M' : b ! e]]
    & \wedge & [[N' [ V' / x ] -->* M']]
    & &
\end{array}
\]

cast lemma:
\[
\begin{array}{rccccc}
    & [[empty |- M <= M' : a ! e]]
    & \wedge & [[cast pa' M --> N]]
    & \wedge & [[commutesl pa' a b]]
\\ \implies \exists [[N']],
    & [[empty |- N <= N' : b ! e]]
    & \wedge & [[M' -->* N']]
    & &
\end{array}
\]

op lemma:
\[
\begin{array}{rccccc}
    & [[empty |- Ex [ op ( V ) ] <= M' : p]]
    & & & &
\\ \implies \exists [[Ex']], [[V']], [[e]],
    & \varnothing \vdash [[Ex]] \le [[Ex']]
    & \wedge & [[empty |- V <= V' : id ! e]]
    & \wedge & [[M' -->* Ex' [ op ( V' ) ] ]]
\end{array}
\]

\begin{figure}
$$
\input{figures/sim}
$$
\caption{Diagram illustrating the simulation lemma}
\label{fig:sim}
\end{figure}

\begin{figure}
$$ \input{figures/catchup} $$
\caption{Diagram illustrating the catchup lemma}
\label{fig:catchup}
\end{figure}
